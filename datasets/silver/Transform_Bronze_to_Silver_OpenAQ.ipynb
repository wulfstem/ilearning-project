{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57cc925-f1d5-4ebd-8883-e8cc697e36d6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-01-08T16:56:57.6330262Z",
       "execution_start_time": "2026-01-08T16:55:15.3305823Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "41975d60-64d0-4a48-a132-7446981515d3",
       "queued_time": "2026-01-08T16:55:03.8306457Z",
       "session_id": "ae59fca5-0667-4e45-b7b3-9803a0e4130f",
       "session_start_time": "2026-01-08T16:55:03.8321571Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, ae59fca5-0667-4e45-b7b3-9803a0e4130f, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading locations...\n",
      "    Found 59 locations\n",
      "\n",
      "Getting sensors...\n",
      "    Found 97 relevant sensors\n",
      "\n",
      "Fetching daily measurements...\n",
      "    Progress: 20/97, 984 measurements...\n",
      "    Progress: 40/97, 984 measurements...\n",
      "    Progress: 60/97, 984 measurements...\n",
      "    Progress: 80/97, 2584 measurements...\n",
      "\n",
      "    Complete! 23/97 sensors, 4058 measurements\n",
      "\n",
      "Creating Silver DataFrame...\n",
      "\n",
      " Data Quality Checks:\n",
      "+----+---------+-------------+---------+\n",
      "|date|value_avg|location_name|parameter|\n",
      "+----+---------+-------------+---------+\n",
      "|   0|        0|            0|        0|\n",
      "+----+---------+-------------+---------+\n",
      "\n",
      "Negative values: 15\n",
      "Extreme values (>500): 14\n",
      "   Records after cleaning: 4,029\n",
      "\n",
      "    Quality Summary:\n",
      "+---------+------------+---------+---------+---------+---------+\n",
      "|parameter|measurements|locations|avg_value|min_value|max_value|\n",
      "+---------+------------+---------+---------+---------+---------+\n",
      "|     pm10|        1503|        8|    12.14|     0.01|   113.78|\n",
      "|     pm25|        2526|        9|     9.16|      0.0|    115.3|\n",
      "+---------+------------+---------+---------+---------+---------+\n",
      "\n",
      "DataFrame: 4,029 measurements\n",
      "\n",
      " Sample:\n",
      "+----------+-------------+---------+---------+\n",
      "|date      |location_name|parameter|value_avg|\n",
      "+----------+-------------+---------+---------+\n",
      "|2022-02-23|Morrisania   |pm25     |7.72     |\n",
      "|2022-02-24|Morrisania   |pm25     |5.29     |\n",
      "|2022-02-25|Morrisania   |pm25     |5.15     |\n",
      "|2022-02-26|Morrisania   |pm25     |6.18     |\n",
      "|2022-02-27|Morrisania   |pm25     |6.71     |\n",
      "|2022-02-28|Morrisania   |pm25     |5.05     |\n",
      "|2022-03-01|Morrisania   |pm25     |7.47     |\n",
      "|2022-03-02|Morrisania   |pm25     |7.74     |\n",
      "|2022-03-03|Morrisania   |pm25     |8.98     |\n",
      "|2022-03-04|Morrisania   |pm25     |8.8      |\n",
      "|2022-03-05|Morrisania   |pm25     |10.15    |\n",
      "|2022-03-06|Morrisania   |pm25     |12.82    |\n",
      "|2022-03-07|Morrisania   |pm25     |10.26    |\n",
      "|2022-03-08|Morrisania   |pm25     |2.67     |\n",
      "|2022-03-09|Morrisania   |pm25     |4.41     |\n",
      "+----------+-------------+---------+---------+\n",
      "only showing top 15 rows\n",
      "\n",
      "\n",
      "    By parameter:\n",
      "+---------+----+---------+-----+----------+----------+\n",
      "|parameter|days|locations|  avg|first_date| last_date|\n",
      "+---------+----+---------+-----+----------+----------+\n",
      "|     pm25|2526|        9| 9.16|2022-02-23|2024-12-31|\n",
      "|     pm10|1503|        8|12.14|2024-03-06|2024-12-31|\n",
      "+---------+----+---------+-----+----------+----------+\n",
      "\n",
      "\n",
      "Saving...\n",
      "    View: silver_openaq_daily\n",
      " \n",
      "OPENAQ SILVER COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "from api_key import MY_API_KEY\n",
    "\n",
    "API_KEY = MY_API_KEY\n",
    "headers = {\"X-API-Key\": API_KEY}\n",
    "\n",
    "print(\"\\nReading locations...\")\n",
    "locations_df = spark.read.parquet(\"Files/bronze/openaq/locations\")\n",
    "locations = locations_df.select(\"id\", \"name\").collect()\n",
    "print(f\"    Found {len(locations)} locations\")\n",
    "\n",
    "print(\"\\nGetting sensors...\")\n",
    "\n",
    "all_sensors = []\n",
    "\n",
    "for location in locations:\n",
    "    location_id = location['id']\n",
    "    location_name = location['name']\n",
    "    \n",
    "    sensors_url = f\"https://api.openaq.org/v3/locations/{location_id}/sensors\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(sensors_url, headers=headers, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            sensors = data.get('results', [])\n",
    "            \n",
    "            for sensor in sensors:\n",
    "                sensor_id = sensor.get('id')\n",
    "                parameter_obj = sensor.get('parameter', {})\n",
    "                parameter_name = parameter_obj.get('name', 'unknown')\n",
    "                \n",
    "                if parameter_name in ['pm25', 'pm10', 'no2', 'o3', 'co', 'so2']:\n",
    "                    all_sensors.append({\n",
    "                        'sensor_id': sensor_id,\n",
    "                        'location_id': location_id,\n",
    "                        'location_name': location_name,\n",
    "                        'parameter': parameter_name\n",
    "                    })\n",
    "        \n",
    "        time.sleep(0.05)\n",
    "                \n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(f\"    Found {len(all_sensors)} relevant sensors\")\n",
    "\n",
    "print(f\"\\nFetching daily measurements...\")\n",
    "\n",
    "all_measurements = []\n",
    "sensors_with_data = 0\n",
    "sensors_processed = 0\n",
    "\n",
    "for sensor in all_sensors:\n",
    "    sensors_processed += 1\n",
    "    \n",
    "    if sensors_processed % 20 == 0:\n",
    "        print(f\"    Progress: {sensors_processed}/{len(all_sensors)}, {len(all_measurements)} measurements...\")\n",
    "    \n",
    "    sensor_id = sensor['sensor_id']\n",
    "    days_url = f\"https://api.openaq.org/v3/sensors/{sensor_id}/days\"\n",
    "    params = {\"date_from\": \"2019-01-01\", \"date_to\": \"2024-12-31\", \"limit\": 1000}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(days_url, headers=headers, params=params, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            results = data.get('results', [])\n",
    "            \n",
    "            if results:\n",
    "                sensors_with_data += 1\n",
    "                \n",
    "                for record in results:\n",
    "                    period = record.get('period', {})\n",
    "                    datetime_from = period.get('datetimeFrom', {})\n",
    "                    \n",
    "                    if isinstance(datetime_from, dict):\n",
    "                        date_str = datetime_from.get('utc', '')\n",
    "                    else:\n",
    "                        date_str = str(datetime_from)\n",
    "                    \n",
    "                    date_value = date_str[:10] if len(date_str) >= 10 else None\n",
    "                    summary = record.get('summary', {})\n",
    "                    \n",
    "                    all_measurements.append({\n",
    "                        'sensor_id': sensor_id,\n",
    "                        'location_id': sensor['location_id'],\n",
    "                        'location_name': sensor['location_name'],\n",
    "                        'parameter': sensor['parameter'],\n",
    "                        'date': date_value,\n",
    "                        'value_avg': summary.get('avg'),\n",
    "                        'value_min': summary.get('min'),\n",
    "                        'value_max': summary.get('max'),\n",
    "                        'value_sd': summary.get('sd'),\n",
    "                        'value_count': record.get('coverage', {}).get('observedCount')\n",
    "                    })\n",
    "        \n",
    "        time.sleep(0.1)\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f\"\\n    Complete! {sensors_with_data}/{len(all_sensors)} sensors, {len(all_measurements)} measurements\")\n",
    "\n",
    "if all_measurements:\n",
    "    print(f\"\\nCreating Silver DataFrame...\")\n",
    "    \n",
    "    df_pandas = pd.DataFrame(all_measurements)\n",
    "    df_spark = spark.createDataFrame(df_pandas)\n",
    "    \n",
    "    df_spark = df_spark \\\n",
    "        .withColumn(\"date\", to_date(col(\"date\"))) \\\n",
    "        .withColumn(\"value_avg\", col(\"value_avg\").cast(DoubleType())) \\\n",
    "        .withColumn(\"value_min\", col(\"value_min\").cast(DoubleType())) \\\n",
    "        .withColumn(\"value_max\", col(\"value_max\").cast(DoubleType())) \\\n",
    "        .withColumn(\"value_sd\", col(\"value_sd\").cast(DoubleType())) \\\n",
    "        .withColumn(\"value_count\", col(\"value_count\").cast(IntegerType())) \\\n",
    "        .withColumn(\"year\", year(\"date\")) \\\n",
    "        .withColumn(\"month\", month(\"date\")) \\\n",
    "        .withColumn(\"day\", dayofmonth(\"date\")) \\\n",
    "        .withColumn(\"dayofweek\", dayofweek(\"date\")) \\\n",
    "        .withColumn(\"dayname\", date_format(\"date\", \"EEEE\")) \\\n",
    "        .withColumn(\"is_weekend\", when(col(\"dayofweek\").isin([1, 7]), True).otherwise(False)) \\\n",
    "        .withColumn(\"unit\", lit(\"µg/m³\")) \\\n",
    "        .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "    \n",
    "    df_spark = df_spark \\\n",
    "        .withColumn(\"value_avg\", round(col(\"value_avg\"), 2)) \\\n",
    "        .withColumn(\"value_min\", round(col(\"value_min\"), 2)) \\\n",
    "        .withColumn(\"value_max\", round(col(\"value_max\"), 2)) \\\n",
    "        .withColumn(\"value_sd\", round(col(\"value_sd\"), 2))\n",
    "    \n",
    "    df_spark = df_spark.filter(col(\"date\").isNotNull())\n",
    "\n",
    "    print(\"\\n Data Quality Checks:\")\n",
    "    null_counts = df_spark.select([\n",
    "        count(when(col(c).isNull(), c)).alias(c) \n",
    "        for c in [\"date\", \"value_avg\", \"location_name\", \"parameter\"]\n",
    "    ])\n",
    "    null_counts.show()\n",
    "\n",
    "    negative_count = df_spark.filter(col(\"value_avg\") < 0).count()\n",
    "    print(f\"Negative values: {negative_count}\")\n",
    "\n",
    "    extreme_count = df_spark.filter(col(\"value_avg\") > 500).count()\n",
    "    print(f\"Extreme values (>500): {extreme_count}\")\n",
    "\n",
    "    df_spark = df_spark.filter(\n",
    "        (col(\"value_avg\") >= 0) & \n",
    "        (col(\"value_avg\") <= 500) &\n",
    "        (col(\"date\").isNotNull())\n",
    "    )\n",
    "\n",
    "    print(f\"   Records after cleaning: {df_spark.count():,}\")\n",
    "\n",
    "    print(f\"\\n    Quality Summary:\")\n",
    "    df_spark.groupBy(\"parameter\").agg(\n",
    "        count(\"*\").alias(\"measurements\"),\n",
    "        countDistinct(\"location_name\").alias(\"locations\"),\n",
    "        round(avg(\"value_avg\"), 2).alias(\"avg_value\"),\n",
    "        round(min(\"value_avg\"), 2).alias(\"min_value\"),\n",
    "        round(max(\"value_avg\"), 2).alias(\"max_value\")\n",
    "    ).show()\n",
    "    \n",
    "    print(f\"DataFrame: {df_spark.count():,} measurements\")\n",
    "    \n",
    "    print(f\"\\n Sample:\")\n",
    "    df_spark.select(\"date\", \"location_name\", \"parameter\", \"value_avg\").orderBy(\"date\").show(15, truncate=False)\n",
    "    \n",
    "    print(f\"\\n    By parameter:\")\n",
    "    df_spark.groupBy(\"parameter\").agg(\n",
    "        count(\"*\").alias(\"days\"),\n",
    "        countDistinct(\"location_name\").alias(\"locations\"),\n",
    "        round(avg(\"value_avg\"), 2).alias(\"avg\"),\n",
    "        min(\"date\").alias(\"first_date\"),\n",
    "        max(\"date\").alias(\"last_date\")\n",
    "    ).orderBy(desc(\"days\")).show()\n",
    "    \n",
    "    print(f\"\\nSaving...\")\n",
    "    \n",
    "    df_spark.write.mode(\"overwrite\").partitionBy(\"year\", \"month\", \"parameter\").format(\"delta\").saveAsTable(\"silver_openaq\")\n",
    "    \n",
    "    df_spark.createOrReplaceTempView(\"silver_openaq_daily\")\n",
    "    print(f\"    View: silver_openaq_daily\")\n",
    "    print(\" \\nOPENAQ SILVER COMPLETE!\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n No data\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "c0717884-df49-4052-a6f0-7b820d4d2773",
    "default_lakehouse_name": "MobilityLakehouse",
    "default_lakehouse_workspace_id": "1c5c8b52-cac8-4458-8854-e182b18bb906",
    "known_lakehouses": [
     {
      "id": "c0717884-df49-4052-a6f0-7b820d4d2773"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
