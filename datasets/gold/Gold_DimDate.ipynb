{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import *\n","from datetime import datetime, timedelta\n","\n","print(\"POPULATING DimDate\")\n","\n","start_date = datetime(2019, 1, 1)\n","end_date = datetime(2024, 12, 31)\n","date_range = [(start_date + timedelta(days=x)) for x in range((end_date - start_date).days + 1)]\n","\n","holidays = {\n","    # 2019\n","    datetime(2019, 1, 1), datetime(2019, 7, 4), datetime(2019, 11, 28), datetime(2019, 12, 25),\n","    # 2020\n","    datetime(2020, 1, 1), datetime(2020, 7, 4), datetime(2020, 11, 26), datetime(2020, 12, 25),\n","    # 2021\n","    datetime(2021, 1, 1), datetime(2021, 7, 4), datetime(2021, 11, 25), datetime(2021, 12, 25),\n","    # 2022\n","    datetime(2022, 1, 1), datetime(2022, 7, 4), datetime(2022, 11, 24), datetime(2022, 12, 25),\n","    # 2023\n","    datetime(2023, 1, 1), datetime(2023, 7, 4), datetime(2023, 11, 23), datetime(2023, 12, 25),\n","    # 2024\n","    datetime(2024, 1, 1), datetime(2024, 7, 4), datetime(2024, 11, 28), datetime(2024, 12, 25),\n","}\n","\n","dim_date_data = [\n","    (\n","        int(d.strftime(\"%Y%m%d\")),     \n","        d,                            \n","        d.year,                         \n","        (d.month - 1) // 3 + 1,    \n","        d.month,                        \n","        d.strftime(\"%B\"),              \n","        d.day,                          \n","        d.isoweekday(),                 \n","        d.strftime(\"%A\"),               \n","        1 if d.isoweekday() >= 6 else 0, \n","        1 if d in holidays else 0,       \n","        d.isocalendar()[1]              \n","    )\n","    for d in date_range\n","]\n","\n","df_dim_date = spark.createDataFrame(\n","    dim_date_data,\n","    [\"DateKey\", \"Date\", \"Year\", \"Quarter\", \"Month\", \"MonthName\", \n","     \"Day\", \"DayOfWeek\", \"DayName\", \"IsWeekend\", \"IsHoliday\", \"WeekOfYear\"]\n",")\n","\n","print(f\"\\nGenerated {df_dim_date.count()} date records\")\n","print(\"\\nSample data:\")\n","df_dim_date.show(10)\n","\n","print(\"\\nWriting to Lakehouse table: DimDate\")\n","df_dim_date.write \\\n","    .format(\"delta\") \\\n","    .mode(\"overwrite\") \\\n","    .saveAsTable(\"DimDate\")\n","\n","print(\"DimDate created successfully!\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"49283aab-0186-4670-bba8-6a9e6799bbcc","normalized_state":"finished","queued_time":"2026-01-05T14:47:31.5508928Z","session_start_time":"2026-01-05T14:47:31.5519122Z","execution_start_time":"2026-01-05T14:47:43.5578854Z","execution_finish_time":"2026-01-05T14:48:09.9597892Z","parent_msg_id":"240de287-03d7-4e80-ba01-d60492e25024"},"text/plain":"StatementMeta(, 49283aab-0186-4670-bba8-6a9e6799bbcc, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["POPULATING DimDate\n\nGenerated 2192 date records\n\nSample data:\n+--------+-------------------+----+-------+-----+---------+---+---------+---------+---------+---------+----------+\n| DateKey|               Date|Year|Quarter|Month|MonthName|Day|DayOfWeek|  DayName|IsWeekend|IsHoliday|WeekOfYear|\n+--------+-------------------+----+-------+-----+---------+---+---------+---------+---------+---------+----------+\n|20190101|2019-01-01 00:00:00|2019|      1|    1|  January|  1|        2|  Tuesday|        0|        1|         1|\n|20190102|2019-01-02 00:00:00|2019|      1|    1|  January|  2|        3|Wednesday|        0|        0|         1|\n|20190103|2019-01-03 00:00:00|2019|      1|    1|  January|  3|        4| Thursday|        0|        0|         1|\n|20190104|2019-01-04 00:00:00|2019|      1|    1|  January|  4|        5|   Friday|        0|        0|         1|\n|20190105|2019-01-05 00:00:00|2019|      1|    1|  January|  5|        6| Saturday|        1|        0|         1|\n|20190106|2019-01-06 00:00:00|2019|      1|    1|  January|  6|        7|   Sunday|        1|        0|         1|\n|20190107|2019-01-07 00:00:00|2019|      1|    1|  January|  7|        1|   Monday|        0|        0|         2|\n|20190108|2019-01-08 00:00:00|2019|      1|    1|  January|  8|        2|  Tuesday|        0|        0|         2|\n|20190109|2019-01-09 00:00:00|2019|      1|    1|  January|  9|        3|Wednesday|        0|        0|         2|\n|20190110|2019-01-10 00:00:00|2019|      1|    1|  January| 10|        4| Thursday|        0|        0|         2|\n+--------+-------------------+----+-------+-----+---------+---+---------+---------+---------+---------+----------+\nonly showing top 10 rows\n\n\nWriting to Lakehouse table: DimDate\nDimDate created successfully!\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"78cb1132-b9ea-40b9-8f14-1d0a57e1d184"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"c0717884-df49-4052-a6f0-7b820d4d2773"}],"default_lakehouse":"c0717884-df49-4052-a6f0-7b820d4d2773","default_lakehouse_name":"MobilityLakehouse","default_lakehouse_workspace_id":"1c5c8b52-cac8-4458-8854-e182b18bb906"}}},"nbformat":4,"nbformat_minor":5}