{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b512161-6fba-43b2-ae12-f6cbe4a8fc51",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-01-08T16:39:46.0942991Z",
       "execution_start_time": "2026-01-08T16:39:32.3938808Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "3746d345-fc97-454b-848b-c822c3a5f7c4",
       "queued_time": "2026-01-08T16:39:19.4490191Z",
       "session_id": "755d2bf2-fb50-46eb-8654-4194d29d6de8",
       "session_start_time": "2026-01-08T16:39:19.4501144Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, 755d2bf2-fb50-46eb-8654-4194d29d6de8, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAQ NYC Air Quality Locations Ingestion\n",
      "\n",
      "Fetching page 1...\n",
      "Fetched 59 locations\n",
      "Total so far: 59\n",
      "API reports 59 total locations in radius\n",
      "Reached last page (page 1)\n",
      "Successfully fetched 59 locations!\n",
      "\n",
      "Columns found: 24\n",
      "   Preview: id, name, locality, timezone, isMobile, isMonitor, instruments, sensors, licenses, bounds...\n",
      "\n",
      "Preview of data:\n",
      "+---+---------------+-------------+----------------------------------------+\n",
      "|id |name           |country_name |locality                                |\n",
      "+---+---------------+-------------+----------------------------------------+\n",
      "|384|CCNY           |United States|New York-Northern New Jersey-Long Island|\n",
      "|386|Susan Wagner   |United States|NULL                                    |\n",
      "|625|Manhattan/IS143|United States|New York-Northern New Jersey-Long Island|\n",
      "|626|Bronx - IS52   |United States|New York-Northern New Jersey-Long Island|\n",
      "|628|Maspeth        |United States|New York-Northern New Jersey-Long Island|\n",
      "|631|Queens         |United States|New York-Northern New Jersey-Long Island|\n",
      "|642|PS 19          |United States|NULL                                    |\n",
      "|648|Bklyn - PS 314 |United States|KINGS                                   |\n",
      "|662|Division Street|United States|NULL                                    |\n",
      "|664|Bklyn - PS274  |United States|New York-Northern New Jersey-Long Island|\n",
      "+---+---------------+-------------+----------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Dropped complex columns: licenses, sensors, instruments, bounds\n",
      "\n",
      "Saving to: Files/bronze/openaq/locations\n",
      "Data saved successfully!\n",
      "Total records: 59\n",
      "Location: Files/bronze/openaq/locations\n",
      "\n",
      "Summary:\n",
      "Unique location IDs: 59\n",
      "Countries: 1\n",
      "Cities/Localities: 5\n",
      "\n",
      "Locations by country:\n",
      "+-------------+-----+\n",
      "| country_name|count|\n",
      "+-------------+-----+\n",
      "|United States|   59|\n",
      "+-------------+-----+\n",
      "\n",
      "\n",
      "Created temp view: bronze_openaq_locations\n",
      "Ingestion complete!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import current_timestamp, lit, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "from datetime import datetime\n",
    "from api_key import MY_API_KEY\n",
    "\n",
    "API_KEY = MY_API_KEY\n",
    "BASE_URL = \"https://api.openaq.org/v3/locations\"\n",
    "\n",
    "params = {\n",
    "    \"limit\": 1000,\n",
    "    \"page\": 1,\n",
    "    \"coordinates\": \"40.7128,-74.0060\",\n",
    "    \"radius\": \"25000\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"X-API-Key\": API_KEY,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "print(\"OpenAQ NYC Air Quality Locations Ingestion\")\n",
    "\n",
    "all_locations = []\n",
    "page = 1\n",
    "max_pages = 10\n",
    "\n",
    "while page <= max_pages:\n",
    "    print(f\"\\nFetching page {page}...\")\n",
    "    params[\"page\"] = page\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(BASE_URL, headers=headers, params=params, timeout=30)\n",
    "        \n",
    "        if response.status_code == 401:\n",
    "            print(\"Authentication failed! Check your API key.\")\n",
    "            break\n",
    "        elif response.status_code == 429:\n",
    "            print(\"Rate limit exceeded. Waiting...\")\n",
    "            import time\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            print(f\"No more data. Stopping at page {page}.\")\n",
    "            break\n",
    "        \n",
    "        all_locations.extend(results)\n",
    "        found_total = data.get('meta', {}).get('found', 'unknown')\n",
    "        print(f\"Fetched {len(results)} locations\")\n",
    "        print(f\"Total so far: {len(all_locations)}\")\n",
    "        print(f\"API reports {found_total} total locations in radius\")\n",
    "        \n",
    "        if len(results) < params[\"limit\"]:\n",
    "            print(f\"Reached last page (page {page})\")\n",
    "            break\n",
    "        \n",
    "        page += 1\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout on page {page}. Retrying...\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        break\n",
    "\n",
    "if all_locations:\n",
    "    print(f\"Successfully fetched {len(all_locations)} locations!\")\n",
    "    \n",
    "    df_pandas = pd.json_normalize(all_locations)\n",
    "    \n",
    "    print(f\"\\nColumns found: {len(df_pandas.columns)}\")\n",
    "    print(f\"   Preview: {', '.join(df_pandas.columns[:10])}...\")\n",
    "    \n",
    "    df_spark = spark.createDataFrame(df_pandas)\n",
    "    \n",
    "    for col_name in df_spark.columns:\n",
    "        if \".\" in col_name:\n",
    "            df_spark = df_spark.withColumnRenamed(col_name, col_name.replace(\".\", \"_\"))\n",
    "    \n",
    "    if 'distance' in df_spark.columns:\n",
    "        try:\n",
    "            df_spark = df_spark.withColumn(\"distance\", col(\"distance\").cast(DoubleType()))\n",
    "        except:\n",
    "            df_spark = df_spark.drop(\"distance\")\n",
    "            print(\"Dropped 'distance' column (unsupported type)\")\n",
    "    \n",
    "    df_spark = df_spark \\\n",
    "        .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "        .withColumn(\"ingestion_date\", lit(datetime.now().strftime(\"%Y-%m-%d\")))\n",
    "    \n",
    "    print(\"\\nPreview of data:\")\n",
    "    df_spark.select(\"id\", \"name\", \"country_name\", \"locality\").show(10, truncate=False)\n",
    "    \n",
    "    columns_to_drop = []\n",
    "    if \"licenses\" in df_spark.columns:\n",
    "        columns_to_drop.append(\"licenses\")\n",
    "    if \"sensors\" in df_spark.columns:\n",
    "        columns_to_drop.append(\"sensors\")\n",
    "    if \"instruments\" in df_spark.columns:\n",
    "        columns_to_drop.append(\"instruments\")\n",
    "    if \"bounds\" in df_spark.columns:\n",
    "        columns_to_drop.append(\"bounds\")\n",
    "    \n",
    "    if columns_to_drop:\n",
    "        df_spark_clean = df_spark.drop(*columns_to_drop)\n",
    "        print(f\"\\nDropped complex columns: {', '.join(columns_to_drop)}\")\n",
    "    else:\n",
    "        df_spark_clean = df_spark\n",
    "    \n",
    "    output_path = \"Files/bronze/openaq/locations\"\n",
    "    print(f\"\\nSaving to: {output_path}\")\n",
    "    \n",
    "    df_spark_clean.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"parquet\") \\\n",
    "        .save(output_path)\n",
    "    \n",
    "    print(f\"Data saved successfully!\")\n",
    "    print(f\"Total records: {df_spark_clean.count()}\")\n",
    "    print(f\"Location: {output_path}\")\n",
    "    \n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Unique location IDs: {df_spark_clean.select('id').distinct().count()}\")\n",
    "    print(f\"Countries: {df_spark_clean.select('country_name').distinct().count()}\")\n",
    "    print(f\"Cities/Localities: {df_spark_clean.select('locality').distinct().count()}\")\n",
    "    \n",
    "    print(\"\\nLocations by country:\")\n",
    "    df_spark_clean.groupBy(\"country_name\").count().orderBy(col(\"count\").desc()).show(10)\n",
    "    \n",
    "    df_spark_clean.createOrReplaceTempView(\"bronze_openaq_locations\")\n",
    "    print(\"\\nCreated temp view: bronze_openaq_locations\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNo data was fetched. Check your API key and connection.\")\n",
    "\n",
    "print(\"Ingestion complete!\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "c0717884-df49-4052-a6f0-7b820d4d2773",
    "default_lakehouse_name": "MobilityLakehouse",
    "default_lakehouse_workspace_id": "1c5c8b52-cac8-4458-8854-e182b18bb906",
    "known_lakehouses": [
     {
      "id": "c0717884-df49-4052-a6f0-7b820d4d2773"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
